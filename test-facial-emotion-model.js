const roboflow = require('roboflow');
const fs = require('fs');
const path = require('path');

async function testFacialEmotionModel() {
    console.log('ğŸ”§ Testing Facial Emotion Recognition Model...');
    console.log('ğŸ¤– Model: uni-o612z/facial-emotion-recognition');
    console.log('ğŸ“Š Detects: angry, happy, sad');
    
    // Use the specific model from Roboflow Universe
    const modelId = 'uni-o612z/facial-emotion-recognition';
    const apiKey = process.env.ROBOFLOW_API_KEY;
    
    if (!apiKey) {
        console.log('âŒ No API key found! Please set ROBOFLOW_API_KEY');
        return;
    }
    
    console.log('ğŸ”‘ Using API key:', apiKey.substring(0, 10) + '...');
    
    // Find the temp directory where frames are saved
    const tempDir = path.join(require('os').tmpdir(), 'coding-buddy-bot');
    console.log('ğŸ“ Looking for frames in:', tempDir);
    
    // Also check a few other possible locations
    const possiblePaths = [
        tempDir,
        path.join(require('os').tmpdir(), 'coding-buddy-webcam'),
        path.join(process.cwd(), 'temp'),
        path.join(process.cwd(), 'frames'),
        '/tmp/coding-buddy-bot',
        '/tmp/coding-buddy-webcam'
    ];
    
    let actualTempDir = null;
    for (const dir of possiblePaths) {
        if (fs.existsSync(dir)) {
            console.log('âœ… Found frame directory:', dir);
            actualTempDir = dir;
            break;
        }
    }
    
    if (!actualTempDir) {
        console.log('âŒ Frame directory not found. Please start the camera first.');
        return;
    }
    
    try {
        // Get all jpg files from the found directory
        const files = fs.readdirSync(actualTempDir).filter(f => f.endsWith('.jpg'));
        console.log(`ğŸ“¸ Found ${files.length} image files`);
        
        if (files.length === 0) {
            console.log('âŒ No image files found. Please start the camera first.');
            return;
        }
        
        // Test with the first few images
        for (let i = 0; i < Math.min(3, files.length); i++) {
            const testImagePath = path.join(actualTempDir, files[i]);
            console.log(`\nğŸ¯ Testing image ${i + 1}: ${files[i]}`);
            console.log('ğŸ“ Image size:', fs.statSync(testImagePath).size, 'bytes');
            
            try {
                console.log('ğŸ”‘ Making API call to facial-emotion-recognition...');
                const result = await roboflow.detectObject(testImagePath, modelId, apiKey);
                console.log('âœ… SUCCESS! Facial emotion detection worked!');
                console.log('ğŸ“Š Full response:', JSON.stringify(result, null, 2));
                
                // Check if any emotions were detected
                if (result.predictions && result.predictions.length > 0) {
                    console.log('ğŸ¯ Emotion Detection Results:');
                    result.predictions.forEach((pred, index) => {
                        console.log(`  ${index + 1}. Emotion: ${pred.class}`);
                        console.log(`     Confidence: ${Math.round(pred.confidence * 100)}%`);
                        if (pred.x && pred.y) {
                            console.log(`     Position: x=${pred.x}, y=${pred.y}, w=${pred.width}, h=${pred.height}`);
                        }
                    });
                    
                    // Map Roboflow emotions to our emotion categories
                    const emotionMap = {
                        'happy': 'happy',
                        'sad': 'frustrated',
                        'angry': 'frustrated'
                    };
                    
                    const bestPrediction = result.predictions.reduce((best, current) => {
                        return current.confidence > best.confidence ? current : best;
                    });
                    
                    const mappedEmotion = emotionMap[bestPrediction.class] || 'focused';
                    console.log(`ğŸ¯ Mapped to extension emotion: ${mappedEmotion}`);
                    
                } else {
                    console.log('âŒ No emotions detected in the image');
                }
                
            } catch (error) {
                console.log('âŒ Failed:', error.message);
                if (error.response) {
                    console.log('   Status:', error.response.status);
                    console.log('   Status Text:', error.response.statusText);
                }
            }
        }
        
        console.log('\nğŸ‰ Facial emotion recognition model test completed!');
        console.log('ğŸ’¡ If successful, you can use this model in your extension.');
        
    } catch (error) {
        console.error('âŒ Error in test:', error.message);
    }
}

testFacialEmotionModel();
